{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pre-trained convnet\n",
    "\n",
    "A common and highly effective approach to deep learning on small image datasets is to leverage a pre-trained network. A pre-trained network \n",
    "is simply a saved network previously trained on a large dataset, typically on a large-scale image classification task. If this original \n",
    "dataset is large enough and general enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a \n",
    "generic model of our visual world, and hence its features can prove useful for many different computer vision problems, even though these \n",
    "new problems might involve completely different classes from those of the original task. For instance, one might train a network on \n",
    "ImageNet (where classes are mostly animals and everyday objects) and then re-purpose this trained network for something as remote as \n",
    "identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning \n",
    "compared to many older shallow learning approaches, and it makes deep learning very effective for small-data problems.\n",
    "\n",
    "In our case, we will consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1000 different classes). \n",
    "\n",
    "La idea principal es utilitzar diferents arquitectures (VGG16, VGG19, ResNet, Inception, Inception-ResNet, Xception) para despues generar un mix que nos permita obtener mejores resultados.\n",
    "\n",
    "There are two ways to leverage a pre-trained network: *feature extraction* and *fine-tuning*. Fine-tuning is much slower and more expensive, but allows \n",
    "us to leverage data augmentation during training: extending the `conv_base` model and running it end-to-end on the inputs. Note that this \n",
    "technique is in fact so expensive that you should only attempt it if you have access to a GPU: it is absolutely intractable on CPU. If you \n",
    "cannot run your code on GPU, then feature extraction is the way to go.\n",
    "\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Fine-tuning consists in unfreezing a few of the top layers \n",
    "of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in our case, the \n",
    "fully-connected classifier) and these top layers. This is called \"fine-tuning\" because it slightly adjusts the more abstract \n",
    "representations of the model being reused, in order to make them more relevant for the problem at hand.\n",
    "\n",
    "![fine-tuning VGG16](https://s3.amazonaws.com/book.keras.io/img/ch5/vgg16_fine_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Freezing\" a layer or set of layers means preventing their weights from getting updated during training. If we don't do this, then the representations that were previously learned by the convolutional base would get modified during training. Since the `Dense` layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.\n",
    "\n",
    "In Keras, freezing a network is done by setting its `trainable` attribute to `False`.    \n",
    "\n",
    "Thus the steps for fine-tuning a network are as follow:\n",
    "\n",
    "* 1) Add your custom network on top of an already trained base network.\n",
    "* 2) Freeze the base network.\n",
    "* 3) Train the part you added.\n",
    "* 4) Unfreeze some layers in the base network.\n",
    "* 5) Jointly train both these layers and the part you added.\n",
    "\n",
    "## Part 1 - Add custom network on top of pretrained network\n",
    "\n",
    "La primera parte se corresponde on los puntos 1, 2 y 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the VGG16 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "\n",
    "conv_base = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(197, 197, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passed three arguments to the constructor:\n",
    "\n",
    "* `weights`, to specify which weight checkpoint to initialize the model from\n",
    "* `include_top`, which refers to including or not the densely-connected classifier on top of the network. By default, this \n",
    "densely-connected classifier would correspond to the 1000 classes from ImageNet. Since we intend to use our own densely-connected \n",
    "classifier (with only two classes, cat and dog), we don't need to include it.\n",
    "* `input_shape`, the shape of the image tensors that we will feed to the network. This argument is purely optional: if we don't pass it, \n",
    "then the network will be able to process inputs of any size.\n",
    "\n",
    "Here's the detail of the architecture of the VGG16 convolutional base: it's very similar to the simple convnets that you are already \n",
    "familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 197, 197, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 99, 99, 64)    9472        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)    (None, 99, 99, 64)    256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 99, 99, 64)    0           bn_conv1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 49, 49, 64)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, 49, 49, 64)    4160        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizatio (None, 49, 49, 64)    256         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 49, 49, 64)    0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, 49, 49, 64)    36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizatio (None, 49, 49, 64)    256         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 49, 49, 64)    0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)          (None, 49, 49, 256)   16640       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, 49, 49, 256)   16640       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizatio (None, 49, 49, 256)   1024        res2a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalization (None, 49, 49, 256)   1024        res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 49, 49, 256)   0           bn2a_branch2c[0][0]              \n",
      "                                                                   bn2a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 49, 49, 256)   0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, 49, 49, 64)    16448       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizatio (None, 49, 49, 64)    256         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 49, 49, 64)    0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, 49, 49, 64)    36928       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizatio (None, 49, 49, 64)    256         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 49, 49, 64)    0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)          (None, 49, 49, 256)   16640       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizatio (None, 49, 49, 256)   1024        res2b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 49, 49, 256)   0           bn2b_branch2c[0][0]              \n",
      "                                                                   activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 49, 49, 256)   0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)          (None, 49, 49, 64)    16448       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizatio (None, 49, 49, 64)    256         res2c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 49, 49, 64)    0           bn2c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)          (None, 49, 49, 64)    36928       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizatio (None, 49, 49, 64)    256         res2c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 49, 49, 64)    0           bn2c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)          (None, 49, 49, 256)   16640       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizatio (None, 49, 49, 256)   1024        res2c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 49, 49, 256)   0           bn2c_branch2c[0][0]              \n",
      "                                                                   activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 49, 49, 256)   0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, 25, 25, 128)   32896       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizatio (None, 25, 25, 128)   512         res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 25, 25, 128)   0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, 25, 25, 128)   147584      activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizatio (None, 25, 25, 128)   512         res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 25, 25, 128)   0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)          (None, 25, 25, 512)   66048       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, 25, 25, 512)   131584      activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizatio (None, 25, 25, 512)   2048        res3a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalization (None, 25, 25, 512)   2048        res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 25, 25, 512)   0           bn3a_branch2c[0][0]              \n",
      "                                                                   bn3a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 25, 25, 512)   0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, 25, 25, 128)   65664       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizatio (None, 25, 25, 128)   512         res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 25, 25, 128)   0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, 25, 25, 128)   147584      activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizatio (None, 25, 25, 128)   512         res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 25, 25, 128)   0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)          (None, 25, 25, 512)   66048       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizatio (None, 25, 25, 512)   2048        res3b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 25, 25, 512)   0           bn3b_branch2c[0][0]              \n",
      "                                                                   activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 25, 25, 512)   0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)          (None, 25, 25, 128)   65664       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizatio (None, 25, 25, 128)   512         res3c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 25, 25, 128)   0           bn3c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)          (None, 25, 25, 128)   147584      activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizatio (None, 25, 25, 128)   512         res3c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 25, 25, 128)   0           bn3c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)          (None, 25, 25, 512)   66048       activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizatio (None, 25, 25, 512)   2048        res3c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 25, 25, 512)   0           bn3c_branch2c[0][0]              \n",
      "                                                                   activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 25, 25, 512)   0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)          (None, 25, 25, 128)   65664       activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizatio (None, 25, 25, 128)   512         res3d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 25, 25, 128)   0           bn3d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)          (None, 25, 25, 128)   147584      activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizatio (None, 25, 25, 128)   512         res3d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 25, 25, 128)   0           bn3d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)          (None, 25, 25, 512)   66048       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizatio (None, 25, 25, 512)   2048        res3d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 25, 25, 512)   0           bn3d_branch2c[0][0]              \n",
      "                                                                   activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 25, 25, 512)   0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, 13, 13, 256)   131328      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizatio (None, 13, 13, 256)   1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 13, 13, 256)   0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, 13, 13, 256)   590080      activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizatio (None, 13, 13, 256)   1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 13, 13, 256)   0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)          (None, 13, 13, 1024)  263168      activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, 13, 13, 1024)  525312      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizatio (None, 13, 13, 1024)  4096        res4a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalization (None, 13, 13, 1024)  4096        res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 13, 13, 1024)  0           bn4a_branch2c[0][0]              \n",
      "                                                                   bn4a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 13, 13, 1024)  0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, 13, 13, 256)   262400      activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizatio (None, 13, 13, 256)   1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 13, 13, 256)   0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, 13, 13, 256)   590080      activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizatio (None, 13, 13, 256)   1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 13, 13, 256)   0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)          (None, 13, 13, 1024)  263168      activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizatio (None, 13, 13, 1024)  4096        res4b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 13, 13, 1024)  0           bn4b_branch2c[0][0]              \n",
      "                                                                   activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 13, 13, 1024)  0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)          (None, 13, 13, 256)   262400      activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizatio (None, 13, 13, 256)   1024        res4c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 13, 13, 256)   0           bn4c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)          (None, 13, 13, 256)   590080      activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizatio (None, 13, 13, 256)   1024        res4c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 13, 13, 256)   0           bn4c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)          (None, 13, 13, 1024)  263168      activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizatio (None, 13, 13, 1024)  4096        res4c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 13, 13, 1024)  0           bn4c_branch2c[0][0]              \n",
      "                                                                   activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 13, 13, 1024)  0           add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)          (None, 13, 13, 256)   262400      activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizatio (None, 13, 13, 256)   1024        res4d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 13, 13, 256)   0           bn4d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)          (None, 13, 13, 256)   590080      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizatio (None, 13, 13, 256)   1024        res4d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 13, 13, 256)   0           bn4d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)          (None, 13, 13, 1024)  263168      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizatio (None, 13, 13, 1024)  4096        res4d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 13, 13, 1024)  0           bn4d_branch2c[0][0]              \n",
      "                                                                   activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 13, 13, 1024)  0           add_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)          (None, 13, 13, 256)   262400      activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizatio (None, 13, 13, 256)   1024        res4e_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 13, 13, 256)   0           bn4e_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)          (None, 13, 13, 256)   590080      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizatio (None, 13, 13, 256)   1024        res4e_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 13, 13, 256)   0           bn4e_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)          (None, 13, 13, 1024)  263168      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizatio (None, 13, 13, 1024)  4096        res4e_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     (None, 13, 13, 1024)  0           bn4e_branch2c[0][0]              \n",
      "                                                                   activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 13, 13, 1024)  0           add_12[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)          (None, 13, 13, 256)   262400      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizatio (None, 13, 13, 256)   1024        res4f_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 13, 13, 256)   0           bn4f_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)          (None, 13, 13, 256)   590080      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizatio (None, 13, 13, 256)   1024        res4f_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 13, 13, 256)   0           bn4f_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)          (None, 13, 13, 1024)  263168      activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizatio (None, 13, 13, 1024)  4096        res4f_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, 13, 13, 1024)  0           bn4f_branch2c[0][0]              \n",
      "                                                                   activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 13, 13, 1024)  0           add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, 7, 7, 512)     524800      activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 7, 7, 512)     0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 7, 7, 512)     0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, 7, 7, 2048)    2099200     activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalization (None, 7, 7, 2048)    8192        res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, 7, 7, 2048)    0           bn5a_branch2c[0][0]              \n",
      "                                                                   bn5a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, 7, 7, 2048)    0           add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, 7, 7, 512)     1049088     activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, 7, 7, 512)     0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_44[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 7, 7, 512)     0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, 7, 7, 2048)    0           bn5b_branch2c[0][0]              \n",
      "                                                                   activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 7, 7, 2048)    0           add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)          (None, 7, 7, 512)     1049088     activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 7, 7, 512)     0           bn5c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 7, 7, 512)     0           bn5c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, 7, 7, 2048)    0           bn5c_branch2c[0][0]              \n",
      "                                                                   activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 7, 7, 2048)    0           add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)      (None, 1, 1, 2048)    0           activation_49[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature map has shape `(4, 4, 512)`. That's the feature on top of which we will stick a densely-connected classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because models behave just like layers, you can add a model (like our `conv_base`) to a `Sequential` model just like you would add a layer. \n",
    "So you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what our model looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 1, 1, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 24,114,826\n",
      "Trainable params: 527,114\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the convolutional base of VGG16 has 14,714,688 parameters, which is very large. The classifier we are adding on top has 2 \n",
    "million parameters.\n",
    "\n",
    "Before we compile and train our model, a very important thing to do is to freeze the convolutional base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this setup, only the weights from the two `Dense` layers that we added will be trained. That's a total of four weight tensors: two per \n",
    "layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, we must first compile the model. \n",
    "If you ever modify weight trainability after compilation, you should then re-compile the model, or these changes would be ignored.\n",
    "\n",
    "Now we can start training our model, with the same data augmentation configuration that we used in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logos/train\n",
      "Found 411 images belonging to 9 classes.\n",
      "Found 121 images belonging to 9 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (None, 10) but got array with shape (20, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a862ea3ac59d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m       \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m       verbose=1)\n\u001b[0m",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1754\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1756\u001b[1;33m             check_batch_axis=True)\n\u001b[0m\u001b[0;32m   1757\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1383\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1384\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    142\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (None, 10) but got array with shape (20, 9)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "base_dir = './logos/'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'test')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "print (train_dir)\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=50,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(197, 197),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(197, 197),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=200,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_resnet50_pre_train.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our results again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f4039b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1225d3b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we reach a validation accuracy of about 96%. This is much better than our small convnet trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Train last layer of pretrained network\n",
    "\n",
    "We have already completed the first 3 steps when doing feature extraction. Let's proceed with the 4th step: we will unfreeze our `conv_base`, \n",
    "and then freeze individual layers inside of it.\n",
    "\n",
    "As a reminder, this is what our convolutional base looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will fine-tune the last 3 convolutional layers, which means that all layers up until `block4_pool` should be frozen, and the layers \n",
    "`block5_conv1`, `block5_conv2` and `block5_conv3` should be trainable.\n",
    "\n",
    "Why not fine-tune more layers? Why not fine-tune the entire convolutional base? We could. However, we need to consider that:\n",
    "\n",
    "* Earlier layers in the convolutional base encode more generic, reusable features, while layers higher up encode more specialized features. It is \n",
    "more useful to fine-tune the more specialized features, as these are the ones that need to be repurposed on our new problem. There would \n",
    "be fast-decreasing returns in fine-tuning lower layers.\n",
    "* The more parameters we are training, the more we are at risk of overfitting. The convolutional base has 15M parameters, so it would be \n",
    "risky to attempt to train it on our small dataset.\n",
    "\n",
    "Thus, in our situation, it is a good strategy to only fine-tune the top 2 to 3 layers in the convolutional base.\n",
    "\n",
    "Let's set this up, starting from where we left off in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start fine-tuning our network. We will do this with the RMSprop optimizer, using a very low learning rate. The reason for using \n",
    "a low learning rate is that we want to limit the magnitude of the modifications we make to the representations of the 3 layers that we are \n",
    "fine-tuning. Updates that are too large may harm these representations.\n",
    "\n",
    "Now let's proceed with fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 959s - loss: 0.1316 - acc: 0.9599 - val_loss: 0.3658 - val_acc: 0.7742\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 907s - loss: 0.0718 - acc: 0.9669 - val_loss: 0.2681 - val_acc: 0.8710\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=32,\n",
    "      epochs=2,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niRGFvohIjCj0RURi5P8DmtdL3vNRD00AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12610d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEaKkLyISIf8fh726CVYPczEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126133470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_curve(points, factor=0.8):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(acc), 'bo', label='Smoothed training acc')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_acc), 'b', label='Smoothed validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(loss), 'bo', label='Smoothed training loss')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_loss), 'b', label='Smoothed validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing a nice 1% absolute improvement.\n",
    "\n",
    "Note that the loss curve does not show any real improvement (in fact, it is deteriorating). You may wonder, how could accuracy improve if the \n",
    "loss isn't decreasing? The answer is simple: what we display is an average of pointwise loss values, but what actually matters for accuracy \n",
    "is the distribution of the loss values, not their average, since accuracy is the result of a binary thresholding of the class probability \n",
    "predicted by the model. The model may still be improving even if this isn't reflected in the average loss.\n",
    "\n",
    "We can now finally evaluate this model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a test accuracy of 97%. In the original Kaggle competition around this dataset, this would have been one of the top results. \n",
    "However, using modern deep learning techniques, we managed to reach this result using only a very small fraction of the training data \n",
    "available (about 10%). There is a huge difference between being able to train on 20,000 samples compared to 2,000 samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-aways: using convnets with small datasets\n",
    "\n",
    "Here's what you should take away from the exercises of these past two sections:\n",
    "\n",
    "* Convnets are the best type of machine learning models for computer vision tasks. It is possible to train one from scratch even on a very \n",
    "small dataset, with decent results.\n",
    "* On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when working with image \n",
    "data.\n",
    "* It is easy to reuse an existing convnet on a new dataset, via feature extraction. This is a very valuable technique for working with \n",
    "small image datasets.\n",
    "* As a complement to feature extraction, one may use fine-tuning, which adapts to a new problem some of the representations previously \n",
    "learned by an existing model. This pushes performance a bit further.\n",
    "\n",
    "Now you have a solid set of tools for dealing with image classification problems, in particular with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
